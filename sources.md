# Sources, pseudoscience, and alternative medicine
Any type of DIY speculation about things like psychiatry runs the risk of being [pseudoscience](https://en.wikipedia.org/wiki/Pseudoscience). I suspect I straddle that line a little, for a few reasons:

1. Things that work for me may not work for most other people with the same apparent condition (i.e. we may have a different latent condition or genetic disposition)
2. I have different goals than many people who want to treat psychiatric disorders, particularly around risks
3. I often deliberately avoid the most effective treatments, as they tend to bypass the most homeostatic regulation mechanisms
4. Medical studies are undergoing a reproducibility crisis, which impacts certain subfields more than others and isn't an excuse to disregard science or results blindly

Here are some examples of these biases.

I have an MTHFR mutation, so a lot of my treatment focuses on preventing homocysteine buildup. This is questionable in medical science; I haven't had any bloodwork done to suggest that I have a problem with homocysteine, nor that my methylation is deficient. I suspect it because I felt much better after taking methylfolate once, and have a history of relevant risk factors like coffee and alcohol use. But I didn't study this in detail; I use methylfolate because it works _for me_ -- then I retroactively speculate about why it worked and see it's likely to create problems. That isn't science, it's post-hoc justification -- and that's ok with me because I'm after quality of life more than quality of research.

(2), (3), and (4) are more interesting and deserve their own sections. First, though, a quick aside about reliability in case it's helpful:


## Finding reliable sources
Wikipedia is great but doesn't have everything. I often end up consulting other websites like [Drugs.com](https://drugs.com) for supplemental information.

I've found [MediaBiasFactCheck.com](https://mediabiasfactcheck.com) to be pretty reliable for determining whether a source is accurate. I occasionally think they may be a little off, for example with [PsychologyToday](https://mediabiasfactcheck.com/psychology-today), which I wonder about sometimes. But overall I think their assessment of sources is pretty accurate and I tend to trust them. I also think it's worth reading [their wikipedia page](https://en.wikipedia.org/wiki/Media_Bias/Fact_Check) for context.

If you know of a better fact-checking strategy, I'd like to hear about it. This is something I haven't thought about as much as I probably should.


## "Safety first" is a conflict of interest (for me)
I have a beef. My expectation is that doctors view me being a potato as better than me being a hyperproductive genius but having a 10% chance of suicide, and I disagree with that assessment. I value my ability to think and at some point I'm willing to roll the dice, probably more than the medical institution would like me to. That's my prerogative because I don't owe life to anyone.

What that means, though, is that I'm not necessarily going to avoid hypomania as assiduously as bipolar medication regimes tend to (despite that being the most effective way to prevent suicide, numerically speaking). In fact, I value hypomania to some extent because it helps me do some of my best work. I'd much rather be able to support consistent hypomania, have some workarounds in place for problem-mania, and avoid depression altogether even though I understand that this probably increases my risk of suicide overall. Quality of life is more important than quantity of life, to me anyway.

I'm certain that I'm not alone in this, which is why I think the medical institution should provide alternatives to the safety-first approach. Studies vary, but something like 60% of bipolar patients are medically noncompliant after two years. This is shockingly bad; if 60% of people stopped wearing seatbelts after two years, we'd ask ourselves how the hell we screwed it up so much. Bipolar is a more complicated problem, of course, but stuff like microdosing OTC lithium and [substituting GABA-ergic supplements for alcohol](alcohol-substitution.md) should be common wisdom in my opinion, especially for people with a history of mental problems.


## ...and on that point, why I'm DIY-ing my medical life
I sometimes think about the commercials where some dude comes on and says "my life is amazing on Awesometophen; talk to your doctor about it." Why does this happen? Why am I recommending medications to my doctor; shouldn't they already know?

Here's what I understand these commercials to mean about our healthcare system.

1. Nobody runs a commercial unless they believe it will work -- and there are companies that measure ad effectiveness, so it's reasonable to assume they do work. Following that logic...
2. Advertising to consumers means more prescriptions.
3. Prescriptions apply to controlled substances; your doctor doesn't prescribe you ibuprofen or some other OTC generic. These substances are controlled because they're dangerous for uninformed users.
4. Doctors are ambivalent enough about what they prescribe that they take drug recommendations from minimally-informed patients.
5. The only way for this not to be understood as malpractice is to assume that doctors really don't know what you need, so your idea carries some merit despite having no rationale behind it.

I don't have a problem with any of this, it's just not the way I want my personal healthcare to work. It's great if my doctor prioritizes my customer satisfaction; but what I want is unbiased expertise from someone who's after long-term outcomes. Given the way we incentivize them, my suspicion is that US healthcare will trade expertise for immediate customer satisfaction whether or it not ends up being good for the customer. Doing anything else works against their economic interest.

To combat this problem, doctors are highly regulated. Drugs need to be backed by efficacy studies and doctors need to stay informed about the literature. All of this is great and introduces another layer of error: my interests aren't necessarily aligned with measured efficacy for reasons I outline in the next section.

By the way, I really don't have a problem with American medicine as in thinking that it's evil or nefarious or something. I got the snip from a qualified urologist and did not attempt it at home (Joyce was insistent on this point). My doctor and I had aligned interests for the procedure and it went great. So I really like US healthcare in the same sense that I really like `goto`: when you need it you need it. But I also think it's the wrong tool for long-term, complex problems that impact quality of life.

...and that's what I'm staking my Wikipedia-fu against. I don't think I know more than doctors do, but I'm willing to say that I can be a better advocate for myself than I expect for them to be. And if I suck as an advocate, I can improve myself more easily than I can improve my doctor. Probably.


## Efficacy studies and homeostatic regulation
My second beef, this time with the way treatment efficacy is reported in literature -- or more specifically, why the most effective treatment can create problems for homeostasis by failing to address the root cause.

Let's take [ADHD](https://en.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder) as an example, and let's suppose the suspected cause is [low dopaminergic functioning](https://en.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder#Genetics). From a clinical perspective, [NDRIs](https://en.wikipedia.org/wiki/Norepinephrine%E2%80%93dopamine_reuptake_inhibitor) are considered highly effective. Rather than increasing dopamine production, they [prevent dopamine reuptake](https://en.wikipedia.org/wiki/Reuptake_inhibitor) -- that is, the same level of dopamine production will result in a higher steady-state level in the brain.

It's easy to see why this works: your brain has very few ways to screw it up, no matter how pathological your dopamine production pipeline is. The flip side is that your brain also has very few ways to downregulate dopamine in the presence of an NDRI, nor is it likely to upregulate the production pipeline given persistently elevated levels. In other words, NDRIs seem to me like a last-resort kind of solution that makes sense when homeostatic regulation is a lost cause.

I have no doubt that some people have this problem and need NDRIs, but I don't want to assume that about myself. I'd rather assume that various enzymes are compromised and try supplementing precursors like tyrosine to see if I can fix it on the production side. That way I don't end up with a brain that has decreased receptor sites as its last-resort downregulation mechanism, which I suspect would result in withdrawal issues -- especially if I do something else later that fixes the production pathway.

The approach I describe will have lower measured efficacy than direct treatments like NDRIs _for the general population_. It relies on additional conditional variables that most studies don't measure, possibly because most patients don't have their genes sequenced before they go in for treatment (and even if they did, there are probably higher-order metabolic interactions that complicate the picture). I don't think it's antiscientific for me to experiment with and use alternative treatments, but I do think it would be antiscientific for doctors to recommend them without a study that justifies their efficacy for the population in question.

**TODO:** fix continuity with the parenthetical below

(I hate to point to publication bias, by the way, because I'm implicitly suggesting that we should change our prior expectations of efficacy to adjust for something we're guessing about. That's obviously a bad methodology because you can confirm whatever you want to. I'm willing to use it because I think I can outperform general-population medical studies by experimentation, but it's still bad science from a methodological perspective.)

Also, in case it's unclear, I don't think it's fair to downgrade treatment options just because they're found to be broadly effective -- like that means they compromise homeostasis by default. That's obviously absurd. I think the right approach is to try to understand _why_ a treatment works and consider the big-picture cost/benefit profile.


## The replication crisis and why I largely ignore it
[There's a replication crisis](https://en.wikipedia.org/wiki/Replication_crisis) affecting social sciences and medical studies. I don't consider this to be a reason to discount science or medicine because alternatives are much worse. It's more about being proactively aware of potential conflicts of interest in study design and trying to understand the mechanisms behind effects that have been observed to see if it passes a sanity check.

I also don't want to understate how serious the replication crisis is for reliability purposes. Numbers vary, but Wikipedia currently describes [reproducibility in medicine](https://en.wikipedia.org/wiki/Replication_crisis#In_medicine) this way (emphasis mine):

> Out of 49 medical studies from 1990–2003 with more than 1000 citations, 45 claimed that the studied therapy was effective. Out of these studies, 16% were contradicted by subsequent studies, 16% had found stronger effects than did subsequent studies, 44% were replicated, and 24% remained largely unchallenged. The US Food and Drug Administration in 1977–1990 found flaws in 10–20% of medical studies. In a paper published in 2012, Glenn Begley, a biotech consultant working at Amgen, and Lee Ellis, at the University of Texas, found that only 11% of 53 pre-clinical cancer studies could be replicated. **The irreproducible studies had a number of features in common, including that studies were not performed by investigators blinded to the experimental versus the control arms, there was a failure to repeat experiments, a lack of positive and negative controls, failure to show all the data, inappropriate use of statistical tests and use of reagents that were not appropriately validated.**

A nontrivial amount of the controversy involves cancer treatments, whose conflict-of-interest profile is pretty different from psychiatric medicine -- especially supplement-based. But it's worth looking out for studies that have questionable features.

For example, I'll go through how I would evaluate [this study on gotu kola and acoustic startle response](https://pubmed.ncbi.nlm.nih.gov/11106141/), which I found after drafting the [strategy section](strategy.md):

+ Conflicts of interest
  + Gotu kola is unregulated, so there's no regulatory incentive to conclude efficacy (good)
  + Gotu kola is a traditional herb that isn't patented, so supply chains have no artificial price inflation (good)
  + Gotu kola isn't the leading product for any company I know of, so I don't think it's going to figure into anyone's sales strategy in an outsized way (good)
  + Traditional medicines are often debunked by studies, so I doubt any researcher is going to tip the scales in favor (good)
  + The sample size is tiny, N = 40, which suggests there wasn't a lot of money that went into it (probably good)
+ Study structure
  + Double-blind (good)
  + Placebo-controlled (good)
  + Concrete hypothesis about receptor bindings up front (good), but some studies have been rewritten after the fact to look this way -- that's part of the replication crisis
  + Prior evidence with mice (good; mice are basically people so this study is pretty redundant)
  + Not-huge sample size (meh)
+ Supporting evidence
  + [CKK receptors](https://en.wikipedia.org/wiki/Cholecystokinin) do in fact relate to anxiety (good)
  + [Wikipedia](https://en.wikipedia.org/wiki/Centella_asiatica) doesn't mention any mechanism of action (meh, but pretty standard for traditional medicines)
+ Reporting
  + "Significantly" without any quantity or p-value attached (bad -- although I suspect/hope the full text has numbers)
  + Measurement methodology is hinted at but not described in detail (meh, although I assume the full text addresses this)
  + Onset and duration are included (good, so I can try it myself)
  + Dosage is included (good, so I can try it)
